{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment-related variables\n",
    "action_size = 4\n",
    "state_size = 33\n",
    "filename = '/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64'\n",
    "env = UnityEnvironment(file_name=filename)\n",
    "brain_name = 'ReacherBrain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "BUFFER_SIZE = 1048576\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99 \n",
    "TAU = 0.001\n",
    "ACTOR_LR = 0.001\n",
    "CRITIC_LR = 0.001\n",
    "WEIGHT_DECAY = 0\n",
    "UPDATE_EVERY = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ornstein-Uhlenbeck noise process by Alexis Cook from Udacity\n",
    "# https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process\"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actor and critic model architecture\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 512)\n",
    "        self.fc2 = nn.Linear(512 + 4, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x, action):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat([x, action],1)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "                           \n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(33, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer by Timo P. Gros\n",
    "# https://github.com/TimoPGros/\n",
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        if done:\n",
    "            done_value = 1\n",
    "        else:\n",
    "            done_value = 0\n",
    "        self.memory.append([state, action, reward, next_state, done_value])\n",
    "    \n",
    "    def sample(self):\n",
    "        samples = (random.sample(self.memory, self.batch_size))\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        \n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "            \n",
    "            states.append(torch.tensor(state).float())\n",
    "            actions.append(torch.tensor(action).float())\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "\n",
    "        states = torch.cat(states).float().view(len(samples), -1)\n",
    "        actions = torch.cat(actions).float().view(len(samples), -1)\n",
    "        rewards = torch.tensor(rewards).float()\n",
    "        next_states = torch.tensor(next_states).float()\n",
    "        dones = torch.tensor(dones).float()\n",
    "        \n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent class largely inspired by Alexis Cook from Udacity\n",
    "# https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "class Agent():\n",
    "    def __init__(self, seed):\n",
    "        self.critic_local = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.actor_local = Actor()\n",
    "        self.actor_target = Actor()\n",
    "\n",
    "        self.seed = random.seed(seed)       \n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=ACTOR_LR)\n",
    "        self.critic_optimizer = optim.Adam(\n",
    "            self.critic_local.parameters(),\n",
    "            lr=CRITIC_LR,\n",
    "            weight_decay=WEIGHT_DECAY\n",
    "        )\n",
    "\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)        \n",
    "        self.noise = OUNoise(4, self.seed)       \n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        is_time_to_update = (self.t_step + 1) % UPDATE_EVERY == 0\n",
    "        if is_time_to_update:\n",
    "            # check if enough experiences in replay buffer\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                samples = self.memory.sample()\n",
    "                self.learn(samples, GAMMA)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.actor_local(state)\n",
    "        action_values += torch.tensor(self.noise.sample()).float()\n",
    "        return np.clip(action_values, -1, 1)    \n",
    "            \n",
    "    def learn(self, samples, gamma):\n",
    "        states, actions, rewards, next_states, dones = samples\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "        # compute targets\n",
    "        q_values_next_states = self.critic_target.forward(\n",
    "            next_states,\n",
    "            self.actor_target(next_states)\n",
    "        )\n",
    "        targets = rewards + (gamma * (q_values_next_states) *  (1 - dones))\n",
    "        predictions = self.critic_local.forward(states, actions)\n",
    "\n",
    "        loss = F.mse_loss(predictions, targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_losses = self.critic_local.forward(states, self.actor_local(states))\n",
    "        actor_loss = - actor_losses.mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)  \n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)  \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                             local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scores over time with box showing parameter set\n",
    "def plot_scores(last_100_scores_means, episode_count, buffer_size, batch_size, lr_actor, lr_critic, tau, actor, critic):\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    textstr = 'max(last_100_scores_means): {}\\nepisode_count: {}\\nbuffer_size: {}\\nbatch_size: {}\\nlr_actor: {}\\nlr_critic: {}\\ntau: {}\\nactor: {}\\ncritic: {} '\n",
    "    textstr = textstr.format(round(np.max(last_100_scores_means), 2), episode_count, buffer_size, batch_size, lr_actor, lr_critic, tau, actor, critic)\n",
    "    ax.plot(np.arange(len(last_100_scores_means)), last_100_scores_means)\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    ax.set_title('Score over number of episodes')\n",
    "    ax.set_xlabel('Episode number')\n",
    "    ax.set_ylabel('Score')\n",
    "    n_ticks = len(last_100_scores_means)\n",
    "    ticks = np.arange(0, n_ticks, 100).astype('int')\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(ticks+100, rotation=45)\n",
    "    filename = 'scores_over_episodes.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DDPG algorithm\n",
    "def run_ddpg(start_time, n_episodes=5000, max_t=1000):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    agent = Agent(0)\n",
    "    scores = []\n",
    "    last_100_scores = deque(maxlen=100)\n",
    "    last_100_scores_means = []\n",
    "    \n",
    "    for episode_count in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        agent.t_step = 0        \n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action.numpy())[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0] \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            minutes_elapsed = round((time.time() - start_time) / 60, 1)\n",
    "            agent.t_step += 1\n",
    "            tracker_str = '\\repisode_count: {}, step_count: {}, last_100_scores_mean: {}, minutes_elapsed: {}'\n",
    "            print(\n",
    "                tracker_str.format(\n",
    "                    episode_count,\n",
    "                    agent.t_step,\n",
    "                    round(np.mean(last_100_scores), 2),\n",
    "                    minutes_elapsed\n",
    "                ),\n",
    "                end=''\n",
    "            )\n",
    "            if done:\n",
    "                break\n",
    "        last_100_scores.append(score)        \n",
    "        scores.append(score)\n",
    "        \n",
    "        if episode_count >= 100:\n",
    "            last_100_scores_means.append(np.mean(last_100_scores))\n",
    "            plot_scores(\n",
    "                last_100_scores_means,\n",
    "                episode_count,\n",
    "                BUFFER_SIZE,\n",
    "                BATCH_SIZE,\n",
    "                ACTOR_LR,\n",
    "                CRITIC_LR,\n",
    "                TAU, \n",
    "                agent.actor_local,\n",
    "                agent.critic_local\n",
    "            )\n",
    "\n",
    "        if np.mean(last_100_scores)>=30.0:\n",
    "            solved_message = '\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\n",
    "            print(solved_message.format(episode_count, np.mean(last_100_scores)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_weights.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from workspace_utils import active_session\n",
    " \n",
    "with active_session():\n",
    "    # track start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # train DDPG agent\n",
    "    run_ddpg(start_time)\n",
    "\n",
    "    # track end time and print training time\n",
    "    end_time = time.time()\n",
    "    training_time = round((end_time - start_time) / 60, 1)\n",
    "    print('\\nTotal training time: {} minutes'.format(training_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
